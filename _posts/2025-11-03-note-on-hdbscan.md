---
layout: post
title: HDBSCANのメモ
date: 2025-11-03 11:25:42 +0900
categories: blog
tags: machine_learning
---

[link](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html)のメモ・・を極めて雑に。

## DBSCAN

core distanceというのを考える。

$$

d_c(x, y) = \mathrm{max}(c_k(x), c_k(y), d(x, h))

$$

ただし、$c_k(x)$は、$x$から$k$番目に近い点の距離。これが大きいということは、この点の周囲にあまり点がない=孤立している。

上の処理は、$x$か$y$が孤立しているなら、その間の距離に下駄を履かせている。つまり、孤立している点は、より孤立するようになる。

- 頂点 = 各点
- 辺 = 全点対間におく
- 辺の重み = 上の$d_c$

としたような重み付きのグラフを考える。このグラフのminimum spanning treeを考える。これのデンドログラムを考えて、適当な高さで切ったのがDBSCAN。

## HDBSCAN

DBSCANのデンドログラムにおいて、「適当な高さ」を高い方から徐々に低くしていく。すると、最初、全点が1つのクラスタだったところから、最後、各点1つずつがクラスタとなるところまで変化する。

このとき、一つのクラスタが分裂する瞬間に着目する。たいていの場合、分裂するときには、「数個の小さいクラスタ」と「大きいクラスタ」に分裂する。ここで、小さい方のサイズが、ある一定の閾値`min cluster size`以下だった場合、そのクラスタは「ノイズ」として切り捨てることにする。この切り落としを、「適当な高さ」を高い方から徐々に低くしていく、過程で一番最後まで行う。

その結果、途中でノイズとして切り捨てられた点と、最後までクラスタの一員として残った点に分かれる。

各クラスタは、全て、最初の一番大きなクラスタ=全点から分裂を繰り返して生まれたもので、親子関係にある。この親子関係の、どの段階を最終的なクラスタとして残すか、が最後の問題。

ここは略。上のドキュメントを読む。[この辺](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html#extract-the-clusters)
