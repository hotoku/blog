---
layout: post
title: Natural Language Processing with Sequence Models week 3
date: 2020-12-04 09:18:42 +0900
tags: coursera
---

## Gradient Descentについて

[このポスト](https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/)
が詳しい。

ロス関数の可視化。[Filter normalization](https://www.cs.umd.edu/~tomg/projects/landscapes/)という
手法があり、ネットワークのアーキテクチャがロス関数の形状に与える影響を見れる。
skip-connectionを導入したり、各層をwideにすることで、ロスがなめらかで凸に近くなる、らしい。

色んなモデルのロス関数を可視化したページ: [https://losslandscape.com/](https://losslandscape.com/)
